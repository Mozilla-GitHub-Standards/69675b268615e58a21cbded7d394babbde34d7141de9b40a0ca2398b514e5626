#! /bin/bash
source etl_env.shinclude
source bin/etl_config.shinclude

# Modify the classpath -- YMMV
# Fortunately the classpath is transmitted to the actual task nodes.

# Get hbase conf on there (e.g. for zk quorum location)
export HADOOP_CLASSPATH="/etc/hbase/conf/:$HADOOP_CLASSPATH"
# Get zookeeper on there (hadoop does not know about zk, but hbase export needs it)
export HADOOP_CLASSPATH="${HBASE_HOME}/lib/zookeeper.jar:$HADOOP_CLASSPATH"
# Google collections API is used by the export task
export HADOOP_CLASSPATH="${HBASE_HOME}/lib/guava-r05.jar:$HADOOP_CLASSPATH"

MAX_INT=2147483647

date_prefix="$(date +'%Y%m%d_%H%M')"
dfspath=$1
tableName=$2
source="hdfs://${hdfs_host}${dfspath}"
echo "SOURCE: $dest"
echo "HADOOP_CLASSPATH: $HADOOP_CLASSPATH"
echo
hadoop jar "${HBASE_HOME}/hbase.jar" import "${tableName}" "${source}"
